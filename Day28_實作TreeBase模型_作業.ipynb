{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 使用樹型模型進行文章分類\n",
    "\n",
    "本次作業主利用[Amazon Review data中的All Beauty](https://nijianmo.github.io/amazon/index.html)來進行review評價分類(文章分類)\n",
    "\n",
    "資料中將review分為1,2,3,4,5分，而在這份作業，我們將評論改分為差評價、普通評價、優良評價(1,2-->1差評、3-->2普通評價、4,5-->3優良評價)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "文本資料較為龐大，這裡我們取前10000筆資料來進行作業練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 1.0,\n",
       " 'verified': True,\n",
       " 'reviewTime': '02 19, 2015',\n",
       " 'reviewerID': 'A1V6B6TNIC10QE',\n",
       " 'asin': '0143026860',\n",
       " 'reviewerName': 'theodore j bigham',\n",
       " 'reviewText': 'great',\n",
       " 'summary': 'One Star',\n",
       " 'unixReviewTime': 1424304000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data\n",
    "all_reviews = []\n",
    "###<your code>###\n",
    "with open('All_Beauty.json','r',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        all_reviews.append(json.loads(line))\n",
    "        \n",
    "all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews[0].get('overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parse label(overall) and corpus(reviewText)\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "###<your code>###\n",
    "for review in all_reviews[:10000]:\n",
    "    if review.get('overall',False) and review.get('reviewText',False):\n",
    "        corpus.append(review['reviewText'])\n",
    "        labels.append(review['overall'])\n",
    "        \n",
    "#transform labels: 1,2 --> 1 and 3 --> 2 and 4,5 --> 3\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 1 or labels[i]==2:\n",
    "        labels[i] = 1\n",
    "    elif labels[i]==3:\n",
    "        labels[i] = 2\n",
    "    else:\n",
    "        labels[i]=3\n",
    "labels[:5]\n",
    "###<your code>###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " \"My  husband wanted to reading about the Negro Baseball and this a great addition to his library\\n Our library doesn't haveinformation so this book is his start. Tthank you\",\n",
       " 'This book was very informative, covering all aspects of game.',\n",
       " 'I am already a baseball fan and knew a bit about the Negro leagues, but I learned a lot more reading this book.',\n",
       " \"This was a good story of the Black leagues. I bought the book to teach in my high school reading class. I found it very informative and exciting. I would recommend to anyone interested in the history of the black leagues. It is well written, unlike a book of facts. The McKissack's continue to write good books for young audiences that can also be enjoyed by adults!\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/linrongwei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preprocessing data\n",
    "#remove email address, punctuations, and change line symbol(\\n)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "#use lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"將pos_tag結果mapping到lemmatizer中pos的格式\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_content(X):\n",
    "    X_clean = [re.sub('[^a-zA-Z]',' ',x).lower() for x in X]#sub取代指定字串\n",
    "    #tokenize\n",
    "    X_token = [nltk.word_tokenize(x) for x in X_clean]\n",
    "    #stopwords_lemmatizer\n",
    "    X_stopwords_lemmatizer = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for content in X_token:\n",
    "        content_clean = []\n",
    "        for word in content:\n",
    "            if word not in stop_words:\n",
    "                word = lemmatizer.lemmatize(word,get_wordnet_pos(word))\n",
    "                content_clean.append(word)\n",
    "        X_stopwords_lemmatizer.append(content_clean)\n",
    "    X_output = [' '.join(x) for x in X_stopwords_lemmatizer]\n",
    "    \n",
    "    return X_output\n",
    "\n",
    "\n",
    "###<your code>###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'husband want reading negro baseball great addition library library haveinformation book start tthank',\n",
       " 'book informative cover aspect game',\n",
       " 'already baseball fan knew bit negro league learn lot reading book',\n",
       " 'good story black league bought book teach high school reading class found informative excite would recommend anyone interested history black league well write unlike book fact mckissack continue write good book young audience also enjoy adult']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = clean_content(corpus)\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7996, 1999, 7996, 1999)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split corpus and label into train and test\n",
    "###<your code>###\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=0)\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change corpus into vector\n",
    "#you can use tfidf or BoW here\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "###<your code>###\n",
    "#tf = TfidfVectorizer()\n",
    "#tf.fit(x_train)\n",
    "#transform training and testing corpus into vector form\n",
    "x_train = cv.fit_transform(x_train).toarray()\n",
    "x_test = cv.fit_transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練與預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7996, 1500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 1500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=4, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=5, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build classification model (decision tree, random forest, or adaboost)\n",
    "#start training\n",
    "forest_cls = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=4,\n",
    "                                           min_samples_split=5, min_samples_leaf=5)\n",
    "'''adaboost_cls = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
    "                                                                        max_depth=3,\n",
    "                                                                        min_samples_split=10,\n",
    "                                                                        min_samples_leaf=5),\n",
    "                                  n_estimators=50,\n",
    "                                  learning_rate=0.8)'''\n",
    "forest_cls.fit(x_train,y_train)\n",
    "###<your code>###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start inference\n",
    "y_pred = forest_cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.896448224112056\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "###<your code>###\n",
    "print('Accuracy:', forest_cls.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       134\n",
      "           2       0.00      0.00      0.00        73\n",
      "           3       0.90      1.00      0.95      1792\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.30      0.33      0.32      1999\n",
      "weighted avg       0.80      0.90      0.85      1999\n",
      "\n",
      "[[   0    0  134]\n",
      " [   0    0   73]\n",
      " [   0    0 1792]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linrongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#calculate confusion matrix, precision, recall, and f1-score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上述資訊可以發現, 模型在好評的準確度高(precision, recall都高), 而在差評的部分表現較不理想, 在普通評價的部分大部分跟差評搞混,\n",
    "同學可以試著學習到的各種方法來提升模型的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
